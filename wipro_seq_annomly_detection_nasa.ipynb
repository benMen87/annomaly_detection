{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIPRO TIME SERIERS - ANOMALY DETECTION AND SEQUENCE PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NASA data \n",
    "\n",
    "[from dataset readme]\n",
    "The data are provided as a zip-compressed text file with 26 columns of numbers, separated by spaces. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:\n",
    "\n",
    "1)  unit number  \n",
    "2)  time, in cycles  \n",
    "3)  operational setting 1  \n",
    "4)  operational setting 2  \n",
    "5)  operational setting 3  \n",
    "6)  sensor measurement  1  \n",
    "7)  sensor measurement  2  \n",
    "...  \n",
    "26) sensor measurement  26  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit number</th>\n",
       "      <th>time in cycles</th>\n",
       "      <th>operational setting 1</th>\n",
       "      <th>operational setting 2</th>\n",
       "      <th>operational setting 3</th>\n",
       "      <th>sensor measurement 1</th>\n",
       "      <th>sensor measurement 2</th>\n",
       "      <th>sensor measurement 3</th>\n",
       "      <th>sensor measurement 4</th>\n",
       "      <th>sensor measurement 5</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor measurement 12</th>\n",
       "      <th>sensor measurement 13</th>\n",
       "      <th>sensor measurement 14</th>\n",
       "      <th>sensor measurement 15</th>\n",
       "      <th>sensor measurement 16</th>\n",
       "      <th>sensor measurement 17</th>\n",
       "      <th>sensor measurement 18</th>\n",
       "      <th>sensor measurement 19</th>\n",
       "      <th>sensor measurement 20</th>\n",
       "      <th>sensor measurement 21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit number  time in cycles  operational setting 1  operational setting 2  \\\n",
       "0            1               1                -0.0007                -0.0004   \n",
       "1            1               2                 0.0019                -0.0003   \n",
       "2            1               3                -0.0043                 0.0003   \n",
       "3            1               4                 0.0007                 0.0000   \n",
       "4            1               5                -0.0019                -0.0002   \n",
       "\n",
       "   operational setting 3  sensor measurement 1  sensor measurement 2  \\\n",
       "0                  100.0                518.67                641.82   \n",
       "1                  100.0                518.67                642.15   \n",
       "2                  100.0                518.67                642.35   \n",
       "3                  100.0                518.67                642.35   \n",
       "4                  100.0                518.67                642.37   \n",
       "\n",
       "   sensor measurement 3  sensor measurement 4  sensor measurement 5  \\\n",
       "0               1589.70               1400.60                 14.62   \n",
       "1               1591.82               1403.14                 14.62   \n",
       "2               1587.99               1404.20                 14.62   \n",
       "3               1582.79               1401.87                 14.62   \n",
       "4               1582.85               1406.22                 14.62   \n",
       "\n",
       "           ...            sensor measurement 12  sensor measurement 13  \\\n",
       "0          ...                           521.66                2388.02   \n",
       "1          ...                           522.28                2388.07   \n",
       "2          ...                           522.42                2388.03   \n",
       "3          ...                           522.86                2388.08   \n",
       "4          ...                           522.19                2388.04   \n",
       "\n",
       "   sensor measurement 14  sensor measurement 15  sensor measurement 16  \\\n",
       "0                8138.62                 8.4195                   0.03   \n",
       "1                8131.49                 8.4318                   0.03   \n",
       "2                8133.23                 8.4178                   0.03   \n",
       "3                8133.83                 8.3682                   0.03   \n",
       "4                8133.80                 8.4294                   0.03   \n",
       "\n",
       "   sensor measurement 17  sensor measurement 18  sensor measurement 19  \\\n",
       "0                    392                   2388                  100.0   \n",
       "1                    392                   2388                  100.0   \n",
       "2                    390                   2388                  100.0   \n",
       "3                    392                   2388                  100.0   \n",
       "4                    393                   2388                  100.0   \n",
       "\n",
       "   sensor measurement 20  sensor measurement 21  \n",
       "0                  39.06                23.4190  \n",
       "1                  39.00                23.4236  \n",
       "2                  38.95                23.3442  \n",
       "3                  38.88                23.3739  \n",
       "4                  38.90                23.4044  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/hillel/projects/time_series/nasa_data/train_FD001.txt'\n",
    "header_names = ['unit number', 'time in cycles']  + ['operational setting %d'%i for i in range(1, 4)] + [ 'sensor measurement %d'%i for i in range(1, 24)]\n",
    "df = pd.read_csv(file_path, sep=' ', header=None, names=header_names)\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_data(df):\n",
    "    \n",
    "    amount_of_units_tot = df['unit number'].max()\n",
    "    amount_of_units_tst = 2\n",
    "    amunt_of_units_tr = amount_of_units_tot - amount_of_units_tst\n",
    "\n",
    "    test_units = np.random.choice(range(1, amount_of_units_tot+1), amount_of_units_tst)\n",
    "    train_units = [i for i in range(1, amount_of_units_tot + 1) if i not in test_units]\n",
    "\n",
    "    X_tr = []\n",
    "    X_tst = []\n",
    "\n",
    "    for idx in range(1, amount_of_units_tot + 1):\n",
    "        group_np = df[df['unit number'] == idx].iloc[:, 1:-1].as_matrix()\n",
    "        me = np.mean(group_np, axis=0)\n",
    "        std = np.std(group_np, axis=0)\n",
    "        group_np_norm = (group_np - me) / (std + 1e-12)\n",
    "        if idx in test_units:\n",
    "            X_tst.append(group_np_norm)\n",
    "        elif idx in train_units:\n",
    "            X_tr.append(group_np_norm)\n",
    "        else:\n",
    "            raise Exception(\"bad index %d ??\"%idx)\n",
    "    return X_tr, X_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Recurrent anttention based model\n",
    "\n",
    "The idea being learn the sequential dependencys and structure via gated rnn,\n",
    "and to learn spatial structure and similarity via attention mechanism. \n",
    "\n",
    "The Attention used in this case is inspired by https://arxiv.org/pdf/1508.04025.pdf\n",
    "Where the context vector is appended to final hiddent state and are passed to output layer.\n",
    "This differs to the standerd case where context is appended to input of the reccurent cell.\n",
    "\n",
    "The RNN sequence is inspired by https://arxiv.org/pdf/1308.0850.pdf\n",
    "Where LSTM based models are used for pedicting next time step.\n",
    "\n",
    "The code is mostly based on code found here:\n",
    "http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainBatch():\n",
    "    \n",
    "    def __init__(self, data, batch_sz, window_sz):\n",
    "        self._data = data\n",
    "        self._batch_sz = batch_sz\n",
    "        self._window_sz = window_sz\n",
    "        self._input_sz = data[0].shape[-1]\n",
    "        self._seq_len = [x.shape[0] for x in self._data]\n",
    "        self._curr_batch = None\n",
    "        \n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        return:batch[(batch_size,seq_len,input_size)]\n",
    "        \"\"\"\n",
    "        batch = np.empty(shape=[self._batch_sz, self._window_sz, self._input_sz]) \n",
    "        amount_of_units = len(self._data)\n",
    "        choose_units = np.random.choice(amount_of_units, self._window_sz)\n",
    "    \n",
    "        for id, unit_id in zip(range(self._batch_sz), choose_units):\n",
    "            w_start_id = np.random.randint(0, self._seq_len[unit_id] - self._window_sz)\n",
    "            unit = self._data[unit_id][w_start_id:w_start_id+self._window_sz, :] # for unit get seq of window sz\n",
    "            batch[id,...] = unit\n",
    "        batch = Variable(torch.from_numpy(batch.astype('float32')), requires_grad=False)\n",
    "        if USE_CUDA:\n",
    "            batch = batch.cuda()\n",
    "        self._curr_data = batch\n",
    "        \n",
    "        return batch   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Recurrent model for capturing sequential information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequencePredRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(SequencePredRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=False)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        output = inputs\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Attention model utilize reccurnt ouputs and their spatial similarity for predicting next time step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttendAndPredict(nn.Module):\n",
    "    \"\"\"Attention nn module that is responsible for computing the alignment scores.\"\"\"\n",
    "\n",
    "    def __init__(self, method, hidden_size, output_size):\n",
    "        super(AttendAndPredict, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Define layers\n",
    "        if self.method == 'general':\n",
    "            self.attention = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attention = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, self.hidden_size))\n",
    "            \n",
    "        self.fc_out = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, hidden, M):\n",
    "        \"\"\"Attend over N rnn sequence prediction outputs till time t-1 (t-2-N...t-1).\n",
    "        \n",
    "        After creating variables to store the attention energies, calculate their \n",
    "        values for each encoder output and return the normalized values.\n",
    "        \n",
    "        Args:\n",
    "            M(mem_size,batch,input size): memory of which to attend over.\n",
    "            hidden(1, batch, input size): hidden state.\n",
    "            \n",
    "        Returns:\n",
    "             Normalized (0..1) energy values, re-sized to 1 x 1 x seq_len\n",
    "        \"\"\"\n",
    "        \n",
    "        mem_size = M.size()[0]\n",
    "        batch_size = M.size()[1]\n",
    "        \n",
    "        # convert to batch first\n",
    "        M = M.permute(1,0,2)\n",
    "        hidden = hidden.permute(1,2,0)\n",
    "\n",
    "        energies = self._score(hidden, M)\n",
    "        a = F.softmax(energies)\n",
    "        c = a.permute(0,2,1).bmm(M)\n",
    "        next_timestep_prediction = self.fc_out(torch.cat((c.squeeze(1), hidden.squeeze(2)), 1))\n",
    "        \n",
    "        return next_timestep_prediction, a\n",
    "        \n",
    "    def _score(self, hidden, M):\n",
    "        \"\"\"\n",
    "        Calculate the relevance of a particular encoder output in respect to the decoder hidden.\n",
    "        Args:\n",
    "            hidden: decoder hidden output used for condition.\n",
    "            M(batch,seq_len,input_size): memory of which to attend over.\n",
    "            hidden(1, batch, input size): hidden state.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.method == 'dot':\n",
    "            # TODO: Not tested\n",
    "            energy = hidden.dot(M)\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attention(M)\n",
    "            energy = torch.bmm(energy, hidden)#hidden.dot(energy)\n",
    "        elif self.method == 'concat':\n",
    "            # TODO: Not tested\n",
    "            energy = self.attention(torch.cat((hidden, M), 1))\n",
    "            energy = self.other.dor(energy)\n",
    "        return energy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SeqRnnAttnAndPred(nn.Module):\n",
    "    \"\"\"\n",
    "    SequenceAttnPred - Recurrent Atteniton based model for predicting next time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size,\n",
    "                 rnn_layers=1, atnn_method='general', memory_size=-1):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        input_size: size of elemnt of sequnece.\n",
    "        hidden_size: size of hidden state of RNN (same as output if only 1 RNN).\n",
    "        output_size: size of output tensor.\n",
    "        rnn_layers: amount of stacked RNN's (see any basic seq2seq paper).\n",
    "        atnn_method: type of attention to use.\n",
    "        memory_size: amount of rnn output's to aggregate and attend over.\n",
    "        \"\"\"\n",
    "        super(SeqRnnAttnAndPred, self).__init__()\n",
    "        self._rnn_layer = SequencePredRNN(input_size, hidden_size, rnn_layers)\n",
    "        self._atnn_layer = AttendAndPredict(atnn_method, hidden_size, output_size)\n",
    "        self._memory_size = memory_size\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        inputs(seq_len,batch,input_len): input sequence predict seq_len + 1\n",
    "        \"\"\"\n",
    "        seq_len = inputs.size()[0]\n",
    "        \n",
    "        # in case of 1 rnn layer output == hidden.\n",
    "        seqrnn_output, hidden = self._rnn_layer(inputs, hidden)\n",
    "        self.memory = seqrnn_output[-self._memory_size-1:-1]\n",
    "        hidden_t = seqrnn_output[-1].unsqueeze(0)\n",
    "        outputs, alignment = self._atnn_layer(hidden_t, self.memory)\n",
    "        return outputs, alignment\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(1, self._batch_size, self._rnn_layer.hidden_size))\n",
    "        hidden = hidden.cuda() if USE_CUDA else hidden\n",
    "        return hidden\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Helper functions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training procedure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(input_seq, model, criterion, train=False, optimizer=None):\n",
    "    \"\"\"\n",
    "    Train for a given sequence batch size.\n",
    "    Args:\n",
    "    input_seq(batch_size,seq_len,input_size): tensor containin sequences.\n",
    "    model: input model - batch is first dim.\n",
    "    criterion: distance measure i.e. l1, l2 etc.\n",
    "    optimizer: GD, ADAM etc.\n",
    "    \"\"\" \n",
    "    input_seq = input_seq.transpose(0, 1)\n",
    "    hidden = model.init_hidden()\n",
    "    preds, alignment = model(input_seq[:-1], hidden)\n",
    "    loss = criterion(preds, input_seq[-1])\n",
    "    \n",
    "    if train:\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.data[0], preds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time remaining given the current time and progress %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full train procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(train_batch, model, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "  \n",
    "\n",
    "    for iter in range(1, n_iters):\n",
    "        \n",
    "        curr_batch = train_batch.next_batch()\n",
    "        \n",
    "        loss, _ = run(curr_batch, model, criterion, True, optimizer)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, float(iter) / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    show_plot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_data, model):\n",
    "    \n",
    "    loss_total = 0  # Reset every print_every\n",
    "    test_count = 0\n",
    "    window_len = 30\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    \n",
    "    for seq in test_data:\n",
    "        seq_len = seq.shape[0]\n",
    "        for i in range(seq_len - window_len):\n",
    "            test_seq = seq[i: i + window_len]\n",
    "            test_seq = Variable(torch.from_numpy(test_seq.astype('float32')), requires_grad=False).unsqueeze(0)\n",
    "            if USE_CUDA:\n",
    "                test_seq = test_seq.cuda()\n",
    "            loss, _ = run(test_seq, model, criterion, False)\n",
    "            loss_total += loss\n",
    "            test_count += 1\n",
    "    print(\"total test error %.4f\"%(float(loss_total)/test_count))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Main Entry Point **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 98 test size: 2\n",
      "0m 31s (- 51m 16s) (5000 0%) 0.1674\n",
      "1m 2s (- 50m 39s) (10000 0%) 0.1424\n",
      "1m 33s (- 50m 19s) (15000 0%) 0.1406\n",
      "2m 4s (- 49m 41s) (20000 0%) 0.1404\n",
      "2m 35s (- 49m 11s) (25000 0%) 0.1398\n",
      "3m 6s (- 48m 39s) (30000 0%) 0.1391\n",
      "3m 37s (- 48m 4s) (35000 0%) 0.1388\n",
      "4m 8s (- 47m 36s) (40000 0%) 0.1387\n",
      "4m 39s (- 47m 7s) (45000 0%) 0.1381\n",
      "5m 10s (- 46m 38s) (50000 0%) 0.1380\n",
      "5m 42s (- 46m 13s) (55000 0%) 0.1379\n",
      "6m 14s (- 45m 49s) (60000 0%) 0.1381\n",
      "6m 47s (- 45m 24s) (65000 0%) 0.1380\n",
      "7m 19s (- 44m 59s) (70000 0%) 0.1380\n",
      "7m 51s (- 44m 32s) (75000 0%) 0.1375\n",
      "8m 23s (- 44m 4s) (80000 0%) 0.1374\n",
      "8m 55s (- 43m 34s) (85000 0%) 0.1372\n",
      "9m 28s (- 43m 7s) (90000 0%) 0.1377\n",
      "9m 59s (- 42m 37s) (95000 0%) 0.1369\n",
      "10m 32s (- 42m 8s) (100000 0%) 0.1373\n",
      "11m 4s (- 41m 39s) (105000 0%) 0.1369\n",
      "11m 36s (- 41m 10s) (110000 0%) 0.1367\n",
      "12m 9s (- 40m 41s) (115000 0%) 0.1374\n",
      "12m 41s (- 40m 11s) (120000 0%) 0.1369\n",
      "13m 14s (- 39m 42s) (125000 0%) 0.1366\n",
      "13m 46s (- 39m 12s) (130000 0%) 0.1366\n",
      "14m 18s (- 38m 42s) (135000 0%) 0.1369\n",
      "14m 51s (- 38m 12s) (140000 0%) 0.1366\n",
      "15m 23s (- 37m 40s) (145000 0%) 0.1369\n",
      "15m 55s (- 37m 9s) (150000 0%) 0.1358\n",
      "16m 27s (- 36m 38s) (155000 0%) 0.1363\n",
      "17m 0s (- 36m 8s) (160000 0%) 0.1362\n",
      "17m 32s (- 35m 37s) (165000 0%) 0.1365\n",
      "18m 5s (- 35m 6s) (170000 0%) 0.1373\n",
      "18m 37s (- 34m 35s) (175000 0%) 0.1362\n",
      "19m 9s (- 34m 3s) (180000 0%) 0.1366\n",
      "19m 41s (- 33m 32s) (185000 0%) 0.1370\n",
      "20m 14s (- 33m 1s) (190000 0%) 0.1360\n",
      "20m 46s (- 32m 29s) (195000 0%) 0.1367\n",
      "21m 18s (- 31m 58s) (200000 0%) 0.1369\n",
      "21m 50s (- 31m 26s) (205000 0%) 0.1365\n",
      "22m 23s (- 30m 54s) (210000 0%) 0.1362\n",
      "22m 55s (- 30m 23s) (215000 0%) 0.1358\n",
      "23m 27s (- 29m 51s) (220000 0%) 0.1371\n",
      "23m 59s (- 29m 19s) (225000 0%) 0.1365\n",
      "24m 32s (- 28m 48s) (230000 0%) 0.1364\n",
      "25m 4s (- 28m 16s) (235000 0%) 0.1363\n",
      "25m 36s (- 27m 44s) (240000 0%) 0.1364\n",
      "26m 8s (- 27m 12s) (245000 0%) 0.1366\n",
      "26m 41s (- 26m 41s) (250000 0%) 0.1359\n",
      "27m 13s (- 26m 9s) (255000 0%) 0.1364\n",
      "27m 45s (- 25m 37s) (260000 0%) 0.1368\n",
      "28m 17s (- 25m 5s) (265000 0%) 0.1363\n",
      "28m 49s (- 24m 33s) (270000 0%) 0.1359\n",
      "29m 21s (- 24m 1s) (275000 0%) 0.1368\n",
      "29m 54s (- 23m 29s) (280000 0%) 0.1357\n",
      "30m 26s (- 22m 57s) (285000 0%) 0.1358\n",
      "30m 58s (- 22m 25s) (290000 0%) 0.1358\n",
      "31m 30s (- 21m 53s) (295000 0%) 0.1362\n",
      "32m 2s (- 21m 21s) (300000 0%) 0.1361\n",
      "32m 34s (- 20m 49s) (305000 0%) 0.1365\n",
      "33m 6s (- 20m 17s) (310000 0%) 0.1364\n",
      "33m 38s (- 19m 45s) (315000 0%) 0.1361\n",
      "34m 10s (- 19m 13s) (320000 0%) 0.1363\n",
      "34m 42s (- 18m 41s) (325000 0%) 0.1362\n",
      "35m 14s (- 18m 9s) (330000 0%) 0.1359\n",
      "35m 46s (- 17m 37s) (335000 0%) 0.1358\n",
      "36m 18s (- 17m 5s) (340000 0%) 0.1360\n",
      "36m 50s (- 16m 32s) (345000 0%) 0.1363\n",
      "37m 22s (- 16m 0s) (350000 0%) 0.1364\n",
      "37m 54s (- 15m 28s) (355000 0%) 0.1355\n",
      "38m 25s (- 14m 56s) (360000 0%) 0.1361\n",
      "38m 58s (- 14m 24s) (365000 0%) 0.1356\n",
      "39m 29s (- 13m 52s) (370000 0%) 0.1361\n",
      "40m 2s (- 13m 20s) (375000 0%) 0.1364\n",
      "40m 32s (- 12m 48s) (380000 0%) 0.1358\n",
      "41m 4s (- 12m 16s) (385000 0%) 0.1361\n",
      "41m 37s (- 11m 44s) (390000 0%) 0.1359\n",
      "42m 9s (- 11m 12s) (395000 0%) 0.1358\n",
      "42m 42s (- 10m 40s) (400000 0%) 0.1362\n",
      "43m 15s (- 10m 8s) (405000 0%) 0.1356\n",
      "43m 47s (- 9m 36s) (410000 0%) 0.1357\n",
      "44m 20s (- 9m 4s) (415000 0%) 0.1362\n",
      "44m 54s (- 8m 33s) (420000 0%) 0.1359\n",
      "45m 24s (- 8m 0s) (425000 0%) 0.1362\n",
      "45m 56s (- 7m 28s) (430000 0%) 0.1354\n",
      "46m 29s (- 6m 56s) (435000 0%) 0.1360\n",
      "47m 2s (- 6m 24s) (440000 0%) 0.1354\n",
      "47m 34s (- 5m 52s) (445000 0%) 0.1354\n",
      "48m 7s (- 5m 20s) (450000 0%) 0.1362\n",
      "48m 40s (- 4m 48s) (455000 0%) 0.1352\n",
      "49m 12s (- 4m 16s) (460000 0%) 0.1355\n",
      "49m 44s (- 3m 44s) (465000 0%) 0.1352\n",
      "50m 16s (- 3m 12s) (470000 0%) 0.1354\n",
      "50m 48s (- 2m 40s) (475000 0%) 0.1352\n",
      "51m 21s (- 2m 8s) (480000 0%) 0.1357\n",
      "51m 54s (- 1m 36s) (485000 0%) 0.1357\n",
      "52m 26s (- 1m 4s) (490000 0%) 0.1359\n",
      "52m 58s (- 0m 32s) (495000 0%) 0.1358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3fac5f310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa398597590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train, data_test = get_train_test_data(df)\n",
    "print(\"train size: %d test size: %d\"%(len(data_train), len(data_test)))\n",
    "input_size = data_train[0].shape[-1]\n",
    "output_size = input_size\n",
    "batch_size = 64\n",
    "hidden_size = input_size # disregarded when rnn_layers=1\n",
    "rnn_layers = 1\n",
    "memory_size = 15\n",
    "window_size = 30 # what is the window we train over\n",
    "\n",
    "model = SeqRnnAttnAndPred(input_size=input_size, hidden_size=hidden_size, output_size=output_size,\n",
    "                          batch_size=batch_size, rnn_layers=rnn_layers, atnn_method='general',\n",
    "                          memory_size=memory_size)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "train_batch = TrainBatch(data_train, batch_size, window_size)\n",
    "\n",
    "trainIters(train_batch, model, 500000, print_every=5000)\n",
    "\n",
    "torch.save(model.state_dict(), \"./saved_models/pred_nasa_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 24)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Run Evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test error 55.1508\n"
     ]
    }
   ],
   "source": [
    "model = SeqRnnAttnAndPred(input_size=input_size, hidden_size=hidden_size, output_size=output_size,\n",
    "                          batch_size=1, rnn_layers=rnn_layers, atnn_method='general',\n",
    "                          memory_size=memory_size)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "model.load_state_dict(torch.load(\"./saved_models/pred_nasa_seq\"))\n",
    "\n",
    "\n",
    "evaluate(data_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
